from datasets import load_dataset
import re

snli = load_dataset("snli")

train_set = snli["train"]
train_set = train_set.filter(lambda x: x["label"] != -1)

negation_words = [
    "not","never","no","none","nothing","is not","are not","was not",
    "were not","does not","do not","did not","has not","have not","had not",
    "cannot","will not","won't","can't","don't","didn't","isn't","aren't"
]

def has_neg(x):
    hyp = x.lower()
    for w in negation_words:
        if re.search(rf"\b{re.escape(w)}\b", hyp):
            return True
    return False

negated_train = train_set.filter(lambda x: has_neg(x["hypothesis"]))

print("Total negated TRAIN examples:", len(negated_train))



-------
neg_sample = negated_train.shuffle(seed=42).select(range(300))
normal_sample = train_set.shuffle(seed=42).select(range(1000))

augmented_train = neg_sample.add_batch(normal_sample)
print("Augmented training size:", len(augmented_train))



-0----=



from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments

model_path = "./trained_model/"
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForSequenceClassification.from_pretrained(model_path)

def tokenize_fn(batch):
    return tokenizer(batch["premise"], batch["hypothesis"],
                     truncation=True, padding="max_length", max_length=128)

tokenized_aug = augmented_train.map(tokenize_fn, batched=True)

training_args = TrainingArguments(
    output_dir="./trained_model_neg_aug",
    num_train_epochs=1,              # Only 1 epoch to avoid overfitting
    per_device_train_batch_size=8,
    learning_rate=1e-5,              # SMALL LR = better generalization
    weight_decay=0.01,
    logging_steps=50
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_aug,
)

trainer.train()

trainer.save_model("./trained_model_neg_aug")
tokenizer.save_pretrained("./trained_model_neg_aug")

