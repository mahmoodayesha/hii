from google.colab import drive
drive.mount('/content/drive')

!unzip "/content/drive/MyDrive/fp-dataset-artifacts-main (1).zip" -d "/content/drive/MyDrive/"

# Commented out IPython magic to ensure Python compatibility.
# %cd "/content/drive/MyDrive/fp-dataset-artifacts-main"

!pip install accelerate datasets torch tqdm evaluate

!python3 run.py --do_train --task nli --dataset snli --output_dir ./trained_model/



!python3 run.py --do_eval --task nli --dataset snli --model ./trained_model/ --output_dir ./eval_output/

!pip install transformers datasets scikit-learn seaborn -q

import torch
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from sklearn.metrics import accuracy_score, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

snli = load_dataset("snli")
test_set = snli['test']

test_set = test_set.filter(lambda x: x['label'] != -1)

print("Total examples after filtering out invalid:", len(test_set))

import re
negation_words = [
    # Adverbs/particles
    "not", "never", "no", "none", "nothing",

    # Negative auxiliary verbs (full forms)
    "is not", "are not", "was not", "were not",
    "does not", "do not", "did not",
    "has not", "have not", "had not",
    "cannot", "will not", "would not",
    "should not", "could not",

    # Negative auxiliary verbs (contractions)
    "isn't", "aren't", "wasn't", "weren't",
    "doesn't", "don't", "didn't",
    "hasn't", "haven't", "hadn't",
    "can't", "won't", "wouldn't", "shouldn't", "couldn't",

    # Negative pronouns
    "nobody", "no one", "neither", "nowhere", "none of"
]

import re

def has_negation(hypothesis):
    hyp = hypothesis.lower()
    # check each negation word as a whole word
    for neg in negation_words:
        # \b ensures it matches only as a whole word
        if re.search(rf'\b{re.escape(neg)}\b', hyp):
            return True
    return False

negated_subset = test_set.filter(lambda x: has_negation(x['hypothesis']))
print(f"Total naturally negated hypotheses: {len(negated_subset)}")

non_negated_subset = test_set.filter(lambda x: not has_negation(x['hypothesis']))
print(f"Total non-negated hypotheses: {len(non_negated_subset)}")

from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch

model_path = "./trained_model/"
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForSequenceClassification.from_pretrained(model_path)

device = "cuda" if torch.cuda.is_available() else "cpu"
model.to(device)
model.eval()

print("num_labels:", model.config.num_labels)

def predict_label(premise, hypothesis):
    inputs = tokenizer(premise, hypothesis, return_tensors="pt", truncation=True, padding=True).to(device)
    with torch.no_grad():
        logits = model(**inputs).logits
    return torch.argmax(logits, dim=-1).item()

# =========================
# 5. Evaluate on NEGATED subset
# =========================
import numpy as np

y_true_neg = []
y_pred_neg = []

for row in negated_subset:
    y_true_neg.append(row["label"])
    y_pred_neg.append(predict_label(row["premise"], row["hypothesis"]))

y_true_neg = np.array(y_true_neg)
y_pred_neg = np.array(y_pred_neg)

acc_negated = accuracy_score(y_true_neg, y_pred_neg)
cm_negated = confusion_matrix(y_true_neg, y_pred_neg, labels=[0, 1, 2])

print("Accuracy on naturally negated hypotheses:", acc_negated)
print("Confusion matrix (rows=true, cols=pred) for negated subset:\n", cm_negated)

labels = ["entailment", "neutral", "contradiction"]

plt.figure(figsize=(6, 5))
sns.heatmap(
    cm_negated,
    annot=True,
    fmt="d",
    cmap="Reds",
    xticklabels=labels,
    yticklabels=labels
)
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title("Confusion Matrix: Negated Subset")
plt.show()

# Show first 10 misclassified examples
misclassified_neg = []
for i, row in enumerate(negated_subset):
    if y_true_neg[i] != y_pred_neg[i]:
        misclassified_neg.append({
            "premise": row["premise"],
            "hypothesis": row["hypothesis"],
            "true_label": int(y_true_neg[i]),
            "pred_label": int(y_pred_neg[i])
        })

print(f"Total misclassified (negated subset): {len(misclassified_neg)}\n")

for example in misclassified_neg[:10]:
    print("Premise: ", example["premise"])
    print("Hypothesis: ", example["hypothesis"])
    print("True label: ", example["true_label"])
    print("Predicted label: ", example["pred_label"])
    print("-" * 50)

# =========================
# 6. Evaluate on NON-NEGATED subset (comparison)
# =========================
y_true_non = []
y_pred_non = []

for row in non_negated_subset:
    y_true_non.append(row["label"])
    y_pred_non.append(predict_label(row["premise"], row["hypothesis"]))

y_true_non = np.array(y_true_non)
y_pred_non = np.array(y_pred_non)

acc_non_negated = accuracy_score(y_true_non, y_pred_non)
cm_non_negated = confusion_matrix(y_true_non, y_pred_non, labels=[0, 1, 2])

print("Accuracy on non-negated hypotheses:", acc_non_negated)
print("Confusion matrix (rows=true, cols=pred) for non-negated subset:\n", cm_non_negated)

plt.figure(figsize=(6, 5))
sns.heatmap(
    cm_non_negated,
    annot=True,
    fmt="d",
    cmap="Blues",
    xticklabels=labels,
    yticklabels=labels
)
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title("Confusion Matrix: Non-Negated Subset")
plt.show()

# =========================
# 7. Breakdown: how often predicted as CONTRADICTION
# =========================
def breakdown_to_contradiction(y_true, y_pred, name: str):
    print(f"\n{name}: how often predicted as CONTRADICTION (label=2)")
    for true_lbl, label_name in zip([0, 1, 2], ["entailment", "neutral", "contradiction"]):
        mask = (y_true == true_lbl)
        if mask.sum() == 0:
            print(f"No examples with true label = {label_name}")
            continue
        total = mask.sum()
        pred_contra = ((y_pred == 2) & mask).sum()
        print(
            f"True {label_name}: {pred_contra}/{total} "
            f"({pred_contra / total:.2%}) predicted as contradiction"
        )

breakdown_to_contradiction(y_true_neg, y_pred_neg, "Negated subset")
breakdown_to_contradiction(y_true_non, y_pred_non, "Non-negated subset")

# =========================
# 8. (Optional) Minimal-pair probe for negation
# =========================
label_map = {0: "entailment", 1: "neutral", 2: "contradiction"}

def show_minimal_pair(premise, hyp1, hyp2):
    l1 = predict_label(premise, hyp1)
    l2 = predict_label(premise, hyp2)
    print("Premise:", premise)
    print("Hyp 1:", hyp1, "->", label_map[l1])
    print("Hyp 2:", hyp2, "->", label_map[l2])
    print("-" * 50)

show_minimal_pair(
    "A woman is laughing.",
    "The woman is laughing.",
    "The woman is not laughing."
)

show_minimal_pair(
    "A man is riding a bike.",
    "The man is riding a bike.",
    "The man is not sleeping."
)
